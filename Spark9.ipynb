{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrQUUSPq9kV6Rm8lWOQjsg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9wb-FcL1WdX4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Start Spark\n",
        "# -----------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALS_Similar_Movies_100_SparkML\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -----------------------------\n",
        "# Load movies.dat\n",
        "# -----------------------------\n",
        "movies = {}\n",
        "with open(\"movies.dat\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "    for line in f:\n",
        "        fields = line.strip().split(\"::\")\n",
        "        movies[int(fields[0])] = fields[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "Fr09hLlKW0qf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Load ratings.dat\n",
        "# -----------------------------\n",
        "ratings_rdd = spark.sparkContext.textFile(\"ratings.dat\")\n",
        "\n",
        "ratings_df = ratings_rdd.map(lambda x: x.split(\"::\")) \\\n",
        "    .map(lambda f: (int(f[0]), int(f[1]), float(f[2]))).toDF([\"userId\", \"movieId\", \"rating\"])\n",
        "\n",
        "# -----------------------------\n",
        "# Train ALS model\n",
        "# -----------------------------\n",
        "als = ALS(\n",
        "    maxIter=10,\n",
        "    regParam=0.05,\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "model = als.fit(ratings_df)\n",
        "\n",
        "# -----------------------------\n",
        "# Prepare item factors for LSH\n",
        "# -----------------------------\n",
        "itemFactors = model.itemFactors.withColumnRenamed(\"id\", \"movieId\")\n",
        "\n",
        "# Convert feature list to ML Vector type (required by LSH)\n",
        "itemFactors = itemFactors.rdd.map(\n",
        "    lambda row: (row.movieId, Vectors.dense(row.features))\n",
        ").toDF([\"movieId\", \"features\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "tmJOh_c3W5GI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Build LSH Model (Spark ML)\n",
        "# -----------------------------\n",
        "lsh = BucketedRandomProjectionLSH(\n",
        "    inputCol=\"features\",\n",
        "    outputCol=\"hashes\",\n",
        "    bucketLength=2.0\n",
        ")\n",
        "\n",
        "lsh_model = lsh.fit(itemFactors)\n",
        "\n",
        "# -----------------------------\n",
        "# Find similar movies (100% Spark ML)\n",
        "# -----------------------------\n",
        "def findSimilarMovies(movieId, topN=10):\n",
        "\n",
        "    target = itemFactors.filter(col(\"movieId\") == movieId)\n",
        "\n",
        "    if target.count() == 0:\n",
        "        print(\"Movie not found:\", movieId)\n",
        "        return\n",
        "\n",
        "    similar = lsh_model.approxNearestNeighbors(\n",
        "        dataset=itemFactors,\n",
        "        key=target.first().features,\n",
        "        numNearestNeighbors=topN + 1 # Get one more to exclude itself later\n",
        "    )\n",
        "\n",
        "    # Remove itself and limit to topN\n",
        "    similar = similar.filter(col(\"movieId\") != movieId)\n",
        "\n",
        "    # Collect results\n",
        "    results = similar.orderBy(\"distCol\").limit(topN).collect()\n",
        "\n",
        "    print(f\"\\nTop {topN} movies similar to: {movies[movieId]}\\n\")\n",
        "\n",
        "    for row in results:\n",
        "        name = movies.get(row.movieId, \"Unknown\")\n",
        "        print(f\"{name}  (distance={row.distCol:.4f})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AEYNYFiYW82Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Test\n",
        "# -----------------------------\n",
        "findSimilarMovies(1, topN=10)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP6OYpx9W_mB",
        "outputId": "be35ae84-29f4-4942-83d6-38c734b090e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 movies similar to: Movie 1 (2001)\n",
            "\n",
            "Movie 13 (2003)  (distance=0.7588)\n",
            "Movie 19 (2009)  (distance=1.5993)\n",
            "Movie 96 (2006)  (distance=2.2107)\n",
            "Movie 99 (2009)  (distance=2.2764)\n",
            "Movie 98 (2008)  (distance=2.5893)\n",
            "Movie 9 (2009)  (distance=2.6385)\n",
            "Movie 53 (2003)  (distance=2.6593)\n",
            "Movie 33 (2003)  (distance=2.7709)\n",
            "Movie 8 (2008)  (distance=2.7831)\n",
            "Movie 58 (2008)  (distance=2.7831)\n"
          ]
        }
      ]
    }
  ]
}